version: '3'

services:

# Setting up Zookeper
  zookeeper:
    image: "bitnami/zookeeper:latest"
    container_name: zookeeper1
    hostname: zookeeper1
    restart: unless-stopped
    ports:
      - "2181:2181"
    environment:
      ALLOW_ANONYMOUS_LOGIN: "yes"

# Setting up Kafdrop to monitor Kafka
  kafdrop:
    image: "obsidiandynamics/kafdrop"
    container_name: kafdrop
    hostname: kafdrop
    restart: unless-stopped
    ports:
      - "9000:9000"
    environment:
        KAFKA_BROKERCONNECT: "kafka:9092"
        JVM_OPTS: "-Xms16M -Xmx48M -Xss180K -XX:-TieredCompilation -XX:+UseStringDeduplication -noverify"

# Setting up Kafka as Messege Queue
  kafka:
    image: "bitnami/kafka:latest"
    container_name: kafka
    hostname: kafka
    ports:
      - "29092:29092" #External Same Host
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_CFG_LISTENERS: "INTERNAL://:9092,EXTERNAL://:29092"
      KAFKA_CFG_ADVERTISED_LISTENERS: "INTERNAL://kafka:9092,EXTERNAL://YourIp:29092"
      KAFKA_CFG_ZOOKEEPER_CONNECT: "zookeeper1:2181"
      KAFKA_INTER_BROKER_LISTENER_NAME: "INTERNAL"
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: "INTERNAL:PLAINTEXT, EXTERNAL:PLAINTEXT"
      ALLOW_PLAINTEXT_LISTENER: "yes"
#      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 #Uncomment for experiments
#      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1 #Uncomment for experiments
    depends_on:
      - zookeeper

# Setting up Spark Master
  spark-master:
    image: docker.io/bitnami/spark:3
    container_name: "spark_master"
    environment:
      SPARK_MODE: "master"
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: "no"
      SPARK_SSL_ENABLED: "no"
    ports:
      - "7077:7077"
      - "8080:8080"

  # Setting up Spark Worker to process jobs
  spark-worker:
    image: docker.io/bitnami/spark:3
    hostname: "worker1"
    container_name: "worker1"
    environment:
      SPARK_MODE: "worker"
      SPARK_MASTER_URL: "spark://spark-master:7077"
      SPARK_WORKER_MEMORY: "4G"
      SPARK_WORKER_CORES: "2"
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: "no"
      SPARK_SSL_ENABLED: "no"
    ports:
      - "8081:8081"
    depends_on:
      - "spark-master"

  # Running python script to process my data from kafka in pyspark
  pyspark-executor:
    build:
      context: ./PySparkExecutor/
      dockerfile: Dockerfile.pysparkexec    
    container_name: executor
  
  # Setting up Big Data Store Cassandra
  cassandra:
    container_name: cassandra
    image: cassandra:4
    ports:
      - "9042:9042"
    volumes:
      - /var/lib/cassandra

  # Setting up Grafana as Monitoring tool for PLC Data
  grafana:
    container_name: grafana
    build:
      context: ./grafana/
      dockerfile: Dockerfile.grafana
    ports:
      - "3000:3000"
    environment: 
      GF_LOG_LEVEL: debug
      GF_INSTALL_PLUGINS: "hadesarchitect-cassandra-datasource"
    volumes:
#      - /home/herb/BA/plc-pipeline/docker_containers/grafana/data:/var/lib/grafana
      - /home/herb/BA/plc-pipeline/docker_containers/grafana/provisioning:/etc/grafana/provisioning/

  # Creating Tables and Keyspaces for Cassandra via SH Script
  plc-data-cassandra:
    build:
      context: ./cassandraExecutor/
      dockerfile: Dockerfile.cassandraexec
    container_name: plc_data_cassandra
    depends_on:
      - cassandra
    restart: "no"
    entrypoint: ["/plc_data_cassandra.sh"]

  # Mosquitto Broker
  mqtt_broker:
    build:
      context: ./mqtt_broker/
      dockerfile: Dockerfile.mqtt
    ports:
      - "1883:1883"
    container_name: mqtt_broker
    volumes:
      - ./mqtt_broker/log/:/mosquitto/log/


volumes:
  grafana:
    external: true