version: '3'

services:

# Setting up Zookeper
  zookeeper:
    image: "bitnami/zookeeper:latest"
    container_name: zookeeper1
    hostname: zookeeper1
    restart: unless-stopped
    ports:
      - "2181:2181"
    environment:
      ALLOW_ANONYMOUS_LOGIN: "yes"

# Setting up Kafdrop to monitor Kafka
  kafdrop:
    image: "obsidiandynamics/kafdrop"
    container_name: kafdrop
    hostname: kafdrop
    restart: unless-stopped
    ports:
      - "9000:9000"
    environment:
        KAFKA_BROKERCONNECT: "kafka:9092"
        JVM_OPTS: "-Xms16M -Xmx48M -Xss180K -XX:-TieredCompilation -XX:+UseStringDeduplication -noverify"

# Setting up Kafka as Messege Queue
  kafka:
    image: "bitnami/kafka:latest"
    container_name: kafka
    hostname: kafka
    ports:
      - "29092:29092" #External Same Host
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_CFG_LISTENERS: "INTERNAL://:9092,EXTERNAL://0.0.0.0:29092"
      KAFKA_CFG_ADVERTISED_LISTENERS: "INTERNAL://kafka:9092,EXTERNAL://localhost:29092"
      KAFKA_CFG_ZOOKEEPER_CONNECT: "zookeeper1:2181"
      KAFKA_INTER_BROKER_LISTENER_NAME: "INTERNAL"
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: "INTERNAL:PLAINTEXT, EXTERNAL:PLAINTEXT"
      ALLOW_PLAINTEXT_LISTENER: "yes"
#      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
#      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    depends_on:
      - zookeeper

# Setting up Spark Master
  spark-master:
    image: docker.io/bitnami/spark:3
    container_name: "spark_master"
    environment:
      SPARK_MODE: "master"
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: "no"
      SPARK_SSL_ENABLED: "no"
    ports:
      - "7077:7077"
      - "8080:8080"

  # Setting up Spark Worker to process jobs
  spark-worker:
    image: docker.io/bitnami/spark:3
    hostname: "worker1"
    container_name: "worker1"
    environment:
      SPARK_MODE: "worker"
      SPARK_MASTER_URL: "spark://spark-master:7077"
      SPARK_WORKER_MEMORY: "4G"
      SPARK_WORKER_CORES: "2"
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: "no"
      SPARK_SSL_ENABLED: "no"
    ports:
      - "8081:8081"
    depends_on:
      - "spark-master"

  # Running my python script to process my data from kafka in pyspark
  pyspark-executor:
    build:
      context: .
      dockerfile: Dockerfile    
    container_name: executor
    volumes:
      - /home/herberto/Schreibtisch/BA/plc-pipeline/src/spark/testSimple/simplestream.py:/scripts/simplestream.py
  
  # Setting up MongoDB
  mongodb:
    image: mongo
    container_name: "mongodb"
    restart: "unless-stopped"
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example
    ports:
      - "27017:27017"

  # Setting up Mongo-Express as Monitoring tool for MongoDB
  #mongo-express:
  #  image: mongo-express:0.54.0
  #  container_name: "mongoexpress"
  #  restart: "unless-stopped"
  #  ports:
  #    - "28081:28081"
  #  environment:
  #    ME_CONFIG_MONGODB_ADMINUSERNAME: root
  #    ME_CONFIG_MONGODB_ADMINPASSWORD: example
# #     ME_CONFIG_MONGODB_URL: "mongodb://root:example@mongo:27017/"
  #    ME_CONFIG_MONGODB_SERVER: "mongodb"
  #  depends_on:
  #    - "mongodb"
  
  # Setting up Grafana as Monitoring tool of Data
  grafana:
    image: grafana/grafana:8.2.2
    container_name: grafana
    hostname: grafana
    ports:
      - "3000:3000"
    environment: 
      GF_SECURITY_DISABLE_INITIAL_ADMIN_CREATION: "true"
      GF_AUTH_ANONYMOUS_ENABLED: "true"
      GF_AUTH_ANONYMOUS_ORG_ROLE: "Admin"
      GF_AUTH_DISABLE_SIGNOUT_MENU: "true"
      GF_AUTH_DISABLE_LOGIN_FORM: "true"
#      GF_INSTALL_PLUGINS: